import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr

# ğŸ” Load Hugging Face token securely
token = os.environ.get("HUGGINGFACE_TOKEN")
if not token:
    raise ValueError("âŒ HUGGINGFACE_TOKEN not found in environment!")

# âœ… Load model and tokenizer
print("[INFO] âœ… Loading tokenizer and model (CPU mode)...")
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", token=token)
model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.3", token=token)

# ğŸ’¬ Chat function
def chat_fn(input_text):
    inputs = tokenizer(input_text, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=200)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# ğŸš€ Launch Gradio UI (this keeps the container alive)
gr.Interface(
    fn=chat_fn,
    inputs="text",
    outputs="text",
    title="ğŸ§  Mistral Chatbot (CPU)"
).launch(server_name="0.0.0.0", server_port=7860, block=True)
